{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from functools import reduce\n",
    "import os\n",
    "from dmba import classificationSummary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#imputation \n",
    "from sklearn.impute import SimpleImputer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d73c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Users/zhao/Desktop/Data_Mining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73110f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not including medications\n",
    "os.chdir('/Users/zhao/Desktop/Data_Mining/data')\n",
    "files= ['demographic.csv', 'diet.csv', 'questionnaire.csv'] #only use demo, diet, and questionnaire \n",
    "demo= pd.read_csv('demographic.csv')\n",
    "diet= pd.read_csv('diet.csv')\n",
    "qr= pd.read_csv('questionnaire.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('demo', demo.shape)\n",
    "print('diet', diet.shape)\n",
    "print('ques', qr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo (10175, 47)\n",
    "diet (9813, 168)\n",
    "ques (10175, 953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for file in files: \n",
    "    df= pd.read_csv(file)\n",
    "    ls.append(df)\n",
    "df_merge= reduce(lambda x,y: pd.merge(x, y,  how='inner', on= 'SEQN', suffixes=('', '_drop')), ls)\n",
    "df_merge.drop([col for col in df_merge.columns if 'drop' in col], axis=1, inplace=True)\n",
    "print(\"merged df shape:\", df_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged df shape: (9813, 1166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238083f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicated SEQN\n",
    "df_merge.SEQN.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e83bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "False    9813\n",
    "Name: SEQN, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbook= pd.read_csv('nhanes_2013_2014_codebook.csv')\n",
    "#convert variable names to upper class to match with df_merge\n",
    "cbook['variable']= cbook['variable'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def94c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all 7s and 9s as null \n",
    "df_merge.replace({7:None, 9:None, 77:None,99:None,777:None,999:None,7777:None,9999:None,77777:None,99999:None,\n",
    "            777777:None,999999:None,55:None,555:None,5555:None,8:None,88:None}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "df_merge.DPQ050.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ad84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<bound method NDFrame.describe of 0       0.0\n",
    "1       0.0\n",
    "2       0.0\n",
    "3       NaN\n",
    "4       3.0\n",
    "       ... \n",
    "9808    0.0\n",
    "9809    NaN\n",
    "9810    NaN\n",
    "9811    NaN\n",
    "9812    NaN\n",
    "Name: DPQ050, Length: 9813, dtype: float64>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create phq9 scores and drop rows with any item missing \n",
    "df_merge['phq9']= df_merge[['DPQ010','DPQ020','DPQ030','DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090']].sum(axis=1, skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee45ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prep for visualization, assign categories to the original data and data after remving missing phq9 scores\n",
    "orig = df_merge #original data set to be used for comparison\n",
    "orig['DataSet'] = np.where(df_merge['phq9'].notna(), 'Cleaned Data', 'Original Data')  #add column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ed8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with any item missing for phq9 \n",
    "\n",
    "df_merge= df_merge[df_merge['phq9'].notna()]\n",
    "print(df_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "(5372, 1168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.phq9.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count    5372.000000\n",
    "mean        3.311616\n",
    "std         4.396482\n",
    "min         0.000000\n",
    "25%         0.000000\n",
    "50%         2.000000\n",
    "75%         5.000000\n",
    "max        27.000000\n",
    "Name: phq9, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e997dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "font1 = {'family':'calibri','color':'darkgray','size':18}\n",
    "font2 = {'family':'calibri','color':'black','size':22}\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "ct = pd.crosstab(orig.RIDRETH3, orig.DataSet)\n",
    "    \n",
    "ax = ct.plot(kind='bar', stacked=True, rot= 45)\n",
    "ax.legend(ncol=1, shadow=True, loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "\n",
    "\n",
    "plt.xlabel(\"Ethnicity\", fontdict=font1)\n",
    "plt.ylabel(\"Count of Records\", fontdict = font1)\n",
    "plt.title(\"Figure 5. Data Set Comparison by Ethnicity\", fontdict = font2)\n",
    "ax.set_xticklabels(['Mexican American', 'Other Hispanic', 'non-H White', 'non-H Black', 'non-H Asian'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "<Figure size 1800x1800 with 0 Axes>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "findfont: Font family ['calibri'] not found. Falling back to DejaVu Sans.\n",
    "findfont: Font family ['calibri'] not found. Falling back to DejaVu Sans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db80b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with more than no missing data\n",
    "df_merge= df_merge.dropna(thresh= 0.8*len(df_merge), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09454271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude non-numeric values\n",
    "df_merge = df_merge.select_dtypes(['number'])\n",
    "print(len(df_merge.columns), 'columns left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "291 columns left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 5372 entries, 0 to 9808\n",
    "Columns: 291 entries, SEQN to phq9\n",
    "dtypes: float64(278), int64(13)\n",
    "memory usage: 12.0 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mode=SimpleImputer(strategy='most_frequent')\n",
    "df_merge = pd.DataFrame(imp_mode.fit_transform(df_merge), columns=df_merge.columns)\n",
    "#could try other imputation method KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57affd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go over the dictionary manually and drop the unwanted variables\n",
    "col = df_merge.columns\n",
    "df_col = cbook[cbook['variable'].isin(col)]\n",
    "df_col.to_csv(r'df_col.csv') #done in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c92d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables that are related to administration/operation (e.g. language of interview, interpreter code)\n",
    "#admin = pd.read_csv('C:/Users/AmroHassan/Google Drive/Classroom/31008 1-Data Mining Principles/Project/Depression-risk-main/Project Data/Depression-risk-main/data/administrative_col.csv')\n",
    "admin = pd.read_csv('administrative_col.csv')\n",
    "admin_col = admin.columns\n",
    "df_merge = df_merge.drop(admin.columns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ed0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables that are collected and calculated after very detailed Dietary Interviews\n",
    "#(e.g. total dietary fiber)\n",
    "#Can keep variables that can be easily answered\n",
    "#(e.g. Total bottled water drank yesterday)\n",
    "#dietary = pd.read_csv('C:/Users/AmroHassan/Google Drive/Classroom/31008 1-Data Mining Principles/Project/Depression-risk-main/Project Data/Depression-risk-main/data/diet_col.csv')\n",
    "dietary = pd.read_csv('diet_col.csv')\n",
    "diet_col = dietary.columns\n",
    "df_merge = df_merge.drop(dietary.columns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab245a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one of HUQ010 and HSD010 (identical question asked in different surveys, highly correlated)\n",
    "df_merge = df_merge.drop('HUQ010', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 5372 entries, 0 to 5371\n",
    "Columns: 211 entries, SEQN to phq9\n",
    "dtypes: float64(211)\n",
    "memory usage: 8.6 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv(r'df_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7184461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71371883",
   "metadata": {},
   "outputs": [],
   "source": [
    "(5372, 211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_merge.phq9, discrete=True)\n",
    "plt.xlabel(\"Numeric PHQ9 Score\", fontdict=font1)\n",
    "plt.ylabel(\"Count of Records\", fontdict = font1)\n",
    "plt.title(\"Figure 2. Distribution of PHQ9 Score\", fontdict = font2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text(0.5, 1.0, 'Figure 2. Distribution of PHQ9 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed17f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['mild_dep']= np.where(df_merge['phq9']>=8,1,0)\n",
    "print('mild', df_merge['mild_dep'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fed7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild 0    4624\n",
    "1     748\n",
    "Name: mild_dep, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = sns.countplot(x='mild_dep', data = df_merge, palette = \"GnBu\")\n",
    "dx.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "dx.set_xlabel(\"PHQ9 Category\", fontdict=font1)\n",
    "dx.set_title('Figure 3. Depression Count (phq9>=8)', y=1.03, fontdict=font2)\n",
    "dx.set_xticklabels(['Non- Dep','Dep'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['mod_dep']= np.where(df_merge['phq9']>=15,1,0)\n",
    "print('modereate', df_merge['mod_dep'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "modereate 0    5177\n",
    "1     195\n",
    "Name: mod_dep, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ee6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = sns.countplot(x='mod_dep', data = df_merge, palette = \"GnBu\")\n",
    "dx.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "dx.set_xlabel(\"PHQ9 Category\", fontdict=font1)\n",
    "dx.set_title('Mod Depression Count', y=1.03, fontdict=font2)\n",
    "dx.set_xticklabels(['Non-Mod Dep','Mod Dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Text(0, 0, 'Non-Mod Dep'), Text(1, 0, 'Mod Dep')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['sev_dep']= np.where(df_merge['phq9']>=20,1,0)\n",
    "print('severe', df_merge['sev_dep'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "severe 0    5315\n",
    "1      57\n",
    "Name: sev_dep, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e81a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = sns.countplot(x='sev_dep', data = df_merge, palette = \"GnBu\")\n",
    "dx.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "dx.set_xlabel(\"PHQ9 Category\", fontdict=font1)\n",
    "dx.set_title('Severe Depression Count', y=1.03, fontdict=font2)\n",
    "dx.set_xticklabels(['Non-Sev Dep','Sev Dep'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['suicidal']= np.where(df_merge['DPQ090']>0,1,0)\n",
    "print('suicidal', df_merge['suicidal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidal 0    5189\n",
    "1     183\n",
    "Name: suicidal, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = sns.countplot(x='suicidal', data = df_merge, palette = \"GnBu\")\n",
    "dx.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "dx.set_xlabel(\"PHQ9 Category\", fontdict=font1)\n",
    "dx.set_title('Suicidal Depression Count', y=1.03, fontdict=font2)\n",
    "dx.set_xticklabels(['Non-Suicidal','Suicidal'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = sns.countplot(x='RIDRETH3', data = df_merge, palette = \"GnBu\")\n",
    "dx.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "dx.set_xlabel(\"Ethnicity\", fontdict=font1)\n",
    "dx.set_title('Records by Ethnicity', y=1.03, fontdict=font2)\n",
    "dx.set_xticklabels(['Mexican American', 'Other Hispanic', 'non-H White', 'non-H Black', 'non-H Asian'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116021a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9))\n",
    "font1 = {'family':'calibri','color':'gray','size':10}\n",
    "font2 = {'family':'calibri','color':'black','size':15}\n",
    "\n",
    "d1=sns.histplot(data=df_merge, x=\"RIDAGEYR\", kde=True, color=\"skyblue\", ax=axs[0, 0])\n",
    "d2=sns.countplot(data=df_merge, x=\"RIAGENDR\", palette=\"GnBu\", ax=axs[0, 1])\n",
    "d3=sns.countplot(data=df_merge, x=\"DMDBORN4\", palette='GnBu', ax=axs[1, 0])\n",
    "d4=sns.countplot(data=df_merge, x=\"DMDEDUC2\", palette=\"GnBu\", ax=axs[1, 1])\n",
    "\n",
    "d1.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "d1.set_xlabel(\"Age in Years\", fontdict=font1)\n",
    "d1.set_title('Age', y=1.03, fontdict=font2)\n",
    "\n",
    "\n",
    "d2.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "d2.set_xlabel(\"Gender\", fontdict=font1)\n",
    "d2.set_title('Gender', y=1.03, fontdict=font2)\n",
    "d2.set_xticklabels(['Male (1)','Female (2)'])\n",
    "\n",
    "d3.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "d3.set_xlabel(\"Country of Birth\", fontdict=font1)\n",
    "d3.set_title('Country of Birth', y=1.03, fontdict=font2)\n",
    "d3.set_xticklabels(['United States (1)','Not in United States (2)'])\n",
    "\n",
    "d4.set_ylabel(\"Count of Records\", fontdict = font1)\n",
    "d4.set_xlabel(\"Level\", fontdict=font1)\n",
    "d4.set_title('Education Attainment', y=1.03, fontdict=font2)\n",
    "d4.set_xticklabels(['less than high school', 'some high school', 'high school graduate/GED or equivalent',  \n",
    "                    'some college or AA degree)', 'college graduate or above'], rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e34d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "findfont: Font family ['calibri'] not found. Falling back to DejaVu Sans.\n",
    "findfont: Font family ['calibri'] not found. Falling back to DejaVu Sans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df241e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "def normalize_data(X): \n",
    "    X_std= pd.DataFrame(StandardScaler().fit_transform(X))\n",
    "    X_std.columns= X.columns\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b746ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "#partition into X and y\n",
    "target=['mild_dep']\n",
    "drop_features= ['phq9',\n",
    "                'SEQN',\n",
    "                'DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060','DPQ070','DPQ080', 'DPQ090',\n",
    "               'mild_dep', 'sev_dep', 'suicidal', 'mod_dep']\n",
    "X = df_merge[df_merge.columns.drop(drop_features)]\n",
    "y= df_merge[target]\n",
    "#normalize predictors\n",
    "X= normalize_data(X)\n",
    "# training and validation sets split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "print('Training_X: ', train_X.shape)\n",
    "print('Validation_X: ', valid_X.shape)\n",
    "print('Training_y: ', train_y.shape)\n",
    "print('Validation_y: ', valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_X:  (3223, 200)\n",
    "Validation_X:  (2149, 200)\n",
    "Training_y:  (3223, 1)\n",
    "Validation_y:  (2149, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(train_X, train_y)\n",
    "importance= xgboost.feature_importances_\n",
    "columns= df_merge.columns\n",
    "df = pd.DataFrame({'feature': train_X.columns, 'importance': importance})\n",
    "df = df.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "[21:49:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select top 30 features \n",
    "df30= df[:30]\n",
    "df30.columns=['variable', 'importance']\n",
    "df30['variable'] = df30['variable'].apply(lambda x: x.upper())\n",
    "df30 = pd.merge(left=df30, right=cbook, left_on='variable', right_on='variable', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "classificationSummary(valid_y, xgboost.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix (Accuracy 0.8809)\n",
    "\n",
    "       Prediction\n",
    "Actual    0    1\n",
    "     0 1803   47\n",
    "     1  209   90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreate X,y for mild depression\n",
    "target=['mild_dep']\n",
    "drop_features= ['phq9',\n",
    "                'SEQN',\n",
    "                'DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060','DPQ070','DPQ080', 'DPQ090',\n",
    "               'mild_dep', 'sev_dep', 'suicidal', 'mod_dep']\n",
    "X = df_merge[df_merge.columns.drop(drop_features)]\n",
    "y= df_merge[target]\n",
    "#normalize predictors\n",
    "X= normalize_data(X)\n",
    "# training and validation sets split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "print('Training_X: ', train_X.shape)\n",
    "print('Validation_X: ', valid_X.shape)\n",
    "print('Training_y: ', train_y.shape)\n",
    "print('Validation_y: ', valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_X:  (3223, 200)\n",
    "Validation_X:  (2149, 200)\n",
    "Training_y:  (3223, 1)\n",
    "Validation_y:  (2149, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1746bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           2774\n",
    "1            449\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41123a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           1850\n",
    "1            299\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac812fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversample the minority class to have 40% the number of examples of the majority class\n",
    "over = SMOTE(sampling_strategy=0.4)\n",
    "#reduce the number of examples in the majority class to have 50% more than the minority class\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y=pipeline.fit_resample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7460a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           2218\n",
    "1           1109\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a36251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set remain the same\n",
    "valid_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           1850\n",
    "1            299\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(train_X, train_y)\n",
    "importance= xgboost.feature_importances_\n",
    "columns= df_merge.columns\n",
    "df = pd.DataFrame({'feature': train_X.columns, 'importance': importance})\n",
    "df = df.sort_values('importance', ascending=False)\n",
    "#select top 20 features \n",
    "df20= df[:20]\n",
    "df20.columns=['variable', 'importance']\n",
    "df20['variable'] = df20['variable'].apply(lambda x: x.upper())\n",
    "df20 = pd.merge(left=df20, right=cbook, left_on='variable', right_on='variable', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4381ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[21:50:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20.to_csv(r'df20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(valid_y, xgboost.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix (Accuracy 0.8809)\n",
    "\n",
    "       Prediction\n",
    "Actual    0    1\n",
    "     0 1775   75\n",
    "     1  181  118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ec52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list= df20.variable.tolist()\n",
    "var_list.append('mild_dep')\n",
    "print(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "['SLQ050', 'PFQ049', 'DLQ040', 'RIAGENDR', 'HSD010', 'DBQ700', 'CBQ550', 'DUQ370', 'SMQ863', 'OHQ770', 'MCQ010', 'DIQ160', 'MCQ080', 'INQ080', 'MCQ160F', 'HSQ510', 'PAQ665', 'DIQ050', 'MCQ160K', 'WHQ040', 'mild_dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5856cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dfs of balanced training data \n",
    "train_y= train_y.filter(var_list)\n",
    "train_X= train_X.filter(var_list)\n",
    "#final testing set \n",
    "valid_y= valid_y.filter(var_list)\n",
    "valid_X= valid_X.filter(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b26d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           2218\n",
    "1           1109\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           1850\n",
    "1            299\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=plt.subplots(figsize=(6,6))\n",
    "corr=train_X.corr()\n",
    "sns.heatmap(corr, cmap=sns.cm.rocket_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767af2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<AxesSubplot:>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f03782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get highly correlated pairs, ranked by absolute correlation values \n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93215d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_abs_correlations(train_X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PFQ049  DLQ040     0.414039\n",
    "HSD010  DBQ700     0.375639\n",
    "PFQ049  HSD010     0.346829\n",
    "SLQ050  PFQ049     0.299231\n",
    "DLQ040  HSD010     0.254703\n",
    "SLQ050  DLQ040     0.249765\n",
    "        HSD010     0.233957\n",
    "HSD010  MCQ080     0.232423\n",
    "MCQ010  MCQ160K    0.232217\n",
    "HSD010  PAQ665     0.228167\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6112a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b875ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(3327, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit=LogisticRegression()\n",
    "logit.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2060b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(train_y, logit.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix (Accuracy 0.7809)\n",
    "\n",
    "       Prediction\n",
    "Actual    0    1\n",
    "     0 1998  220\n",
    "     1  509  600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd806727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "#cross-validation \n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(logit, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a962295",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.778 (0.017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ee7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef= logit.coef_\n",
    "coef= coef.flatten()\n",
    "columns= train_X.columns\n",
    "df = pd.DataFrame({'feature': columns, \n",
    "                   'coefficients': coef})\n",
    "df = df.sort_values('coefficients', ascending=False)\n",
    "df.columns=['variable', 'coefficients']\n",
    "df['variable'] = df['variable'].apply(lambda x: x.upper())\n",
    "df = pd.merge(left=df, right=cbook, left_on='variable', right_on='variable', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4484e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logitsm= sm.Logit(train_y, train_X).fit()\n",
    "logitsm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimization terminated successfully.\n",
    "         Current function value: 0.578656\n",
    "         Iterations 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=[6,6])\n",
    "clf= logit\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC of logistic regression with 10-fold cross validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV \n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': list(range(2,10)), \n",
    "    'min_samples_split': list(range(2,6))\n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, \n",
    "                          n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "print('initial score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "dtree = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47421262",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial score:  0.8491760933866196\n",
    "parameters:  {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 4, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation using training set\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(dtree, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.857 (0.017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=[6,6])\n",
    "clf= dtree\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC of decision tree classifier with 10-fold cross validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV \n",
    "\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': list(range(2, 100)),\n",
    "    'min_samples_split': [3,4,5,7]\n",
    "}\n",
    "gridSearch = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, \n",
    "                          n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "print('initial score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "rf = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793362ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial score:  0.8708166813429971\n",
    "parameters:  {'max_leaf_nodes': 98, 'min_samples_split': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation using training set\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(rf, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.885 (0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a779528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=[6,6])\n",
    "clf= rf\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('random forest classification')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dcf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [40, 50, 60],\n",
    "    'learning_rate': [1, 1.2, 1.5, 1.9]\n",
    "}\n",
    "gridSearch = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid, cv=5, \n",
    "                          n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "print('initial score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "ada = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9195818",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial score:  0.8747381968434599\n",
    "parameters:  {'learning_rate': 1.5, 'n_estimators': 60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bad623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation using training set\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ada, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3631997",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.889 (0.012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=[6,6])\n",
    "clf= ada\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('adaptive boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [60, 70, 80],\n",
    "    'max_depth': [ 7,8,9], \n",
    "    'min_impurity_decrease': [0.1, 0.2, 0.3]\n",
    "}\n",
    "gridSearch = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid, cv=5, \n",
    "                          n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "print('initial score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "gradient = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial score:  0.8726202894623947\n",
    "parameters:  {'max_depth': 7, 'min_impurity_decrease': 0.1, 'n_estimators': 60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2601cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation using training set\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(gradient, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb61079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.887 (0.011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=[6,6])\n",
    "clf= gradient\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('gradient boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [60, 70, 80],\n",
    "    'max_depth': [ 7,8,9], \n",
    "    'min_impurity_decrease': [0.1, 0.2, 0.3]\n",
    "}\n",
    "gridSearch = GridSearchCV(XGBClassifier(random_state=42), param_grid, cv=5, \n",
    "                          n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "print('initial score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "xgboost = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[22:00:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:00:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "initial score:  0.8636094741357899\n",
    "parameters:  {'max_depth': 7, 'min_impurity_decrease': 0.1, 'n_estimators': 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation using training set\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# evaluate model\n",
    "scores = cross_val_score(xgboost, train_X,train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.883 (0.014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.figure(figsize=[6,6])\n",
    "clf= xgboost\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "for train,test in cv.split(train_X,train_y):\n",
    "    prediction = clf.fit(train_X.iloc[train],train_y.iloc[train]).predict_proba(train_X.iloc[test])\n",
    "    fpr, tpr, t = roc_curve(train_y.iloc[test], prediction[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.3f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('xgboost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[22:01:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[22:01:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
    "Parameters: { \"min_impurity_decrease\" } might not be used.\n",
    "\n",
    "  This could be a false alarm, with some parameters getting used by language bindings but\n",
    "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
    "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
    "\n",
    "\n",
    "[22:01:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53510783",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_dep\n",
    "0           1850\n",
    "1            299\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a29298",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2149, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain the selected model with full training set \n",
    "rf= sklearn.base.clone(rf)\n",
    "rf.fit(train_X, train_y)\n",
    "#test in the hold-out test set\n",
    "classificationSummary(valid_y, rf.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0934e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix (Accuracy 0.8818)\n",
    "\n",
    "       Prediction\n",
    "Actual    0    1\n",
    "     0 1769   81\n",
    "     1  173  126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred_proba = rf.predict_proba(valid_X)[::,1]\n",
    "\n",
    "#calculate AUC of model\n",
    "auc = metrics.roc_auc_score(valid_y, y_pred_proba)\n",
    "\n",
    "#print AUC score\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8211289885202928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20683133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(rf, valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fba004ce520>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance,names,model_type):\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add codebook names \n",
    "importance= rf.feature_importances_\n",
    "columns= train_X.columns\n",
    "df = pd.DataFrame({'feature': columns, \n",
    "                   'importance': importance})\n",
    "df = df.sort_values('importance', ascending=False)\n",
    "df.columns=['variable', 'importance']\n",
    "df['variable'] = df['variable'].apply(lambda x: x.upper())\n",
    "df = pd.merge(left=df, right=cbook, left_on='variable', right_on='variable', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(rf.feature_importances_,df.label,'RANDOM FOREST ')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
